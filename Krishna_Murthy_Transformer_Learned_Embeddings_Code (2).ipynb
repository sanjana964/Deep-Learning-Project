{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnmWAf1vb1B1"
      },
      "outputs": [],
      "source": [
        "#Importing and Modifying Training & Testing Data\n",
        "\n",
        "#Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Modules\n",
        "import gensim.models\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
        "\n",
        "#Deepti Import Data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/medical_tc_train.csv')\n",
        "testdataset = pd.read_csv('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/medical_tc_test.csv')\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "modelemb = Word2Vec.load('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/w2v_OA_CR_300d.bin')\n",
        "\n",
        "dataset = pd.DataFrame.to_numpy(dataset)\n",
        "testdataset = pd.DataFrame.to_numpy(testdataset)\n",
        "\n",
        "#In training dataset, make each class have 1100 abstract samples to even out classes\n",
        "dataset = dataset[dataset[:, 0].argsort()]\n",
        "datasetnew=np.copy(dataset[:5500,:])\n",
        "\n",
        "Dofstarts=dict()\n",
        "for i in range(len(dataset)):\n",
        "  row=dataset[i]\n",
        "  if int(row[0]) not in Dofstarts:\n",
        "    Dofstarts[int(row[0])] = i\n",
        "\n",
        "for i in range(5):\n",
        "  i = i+1\n",
        "  valtostart=Dofstarts[i]\n",
        "  datasetnew[(i-1)*1100:i*1100,:] = dataset[valtostart:valtostart+1100,:]\n",
        "\n",
        "dataset = datasetnew"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch\n",
        "pip install gensim\n",
        "pip install torch.utils"
      ],
      "metadata": {
        "id": "ro8vrqM_wftV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.checkpoint\n",
        "import gensim.models"
      ],
      "metadata": {
        "id": "1ox1ClWL6PvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing and Modifying Training & Testing Data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#modules\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
        "\n",
        "#Deepti Import Data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Deep Learning Project 2/medical_tc_train.csv')\n",
        "testdataset = pd.read_csv('/content/drive/MyDrive/Deep Learning Project 2/medical_tc_test.csv')\n",
        "\n",
        "#wordembeddings = pd.read_csv('/content/drive/MyDrive/Deep Learning Project 2/w2v_100d_oa_all.tar.gz')\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "modelemb = Word2Vec.load('/content/drive/MyDrive/Deep Learning Project 2/w2v_OA_CR_300d.bin')\n",
        "modelemb.wv.has_index_for('diabetes')\n",
        "\n",
        "modelemb.wv.word_vec('cardiac_arrest')\n",
        "modelemb.wv.word_vec('lymphangioleiomyomatosis')\n",
        "\n",
        "dataset = pd.DataFrame.to_numpy(dataset)\n",
        "testdataset = pd.DataFrame.to_numpy(testdataset)\n",
        "\n",
        "dataset = dataset[dataset[:, 0].argsort()]\n",
        "\n",
        "datasetnew=np.copy(dataset[:5500,:])\n",
        "\n",
        "Dofstarts=dict()\n",
        "for i in range(len(dataset)):\n",
        "  row=dataset[i]\n",
        "  if int(row[0]) not in Dofstarts:\n",
        "    Dofstarts[int(row[0])] = i\n",
        "\n",
        "for i in range(5):\n",
        "  i = i+1\n",
        "  valtostart=Dofstarts[i]\n",
        "\n",
        "  datasetnew[(i-1)*1100:i*1100,:] = dataset[valtostart:valtostart+1100,:]\n",
        "\n",
        "dataset=datasetnew"
      ],
      "metadata": {
        "id": "Nb2sgmYuhGMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Tokenization\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "#tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "########## ADDED: instead of using only spaces, we also use \"-\" for tokenization!\n",
        "\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Create a custom tokenizer - base code from chat gpt for the tokenizer\n",
        "basic_english_tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Define a custom tokenizer that also splits on dashes\n",
        "def custom_tokenizer(text):\n",
        "    # Use the basic_english tokenizer and split on dashes\n",
        "    tokens = basic_english_tokenizer(text)\n",
        "    # Flatten the list of tokens and split on dashes\n",
        "    tokens = [subtoken for token in tokens for subtoken in token.split(\"-\")]\n",
        "    return tokens\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a test-sentence with a dash.\"\n",
        "tokens = custom_tokenizer(text)\n",
        "tokenizer=custom_tokenizer\n",
        "\n",
        "##############\n",
        "\n",
        "traindata = dataset[:,1]\n",
        "trainlabel = dataset[:,0]\n",
        "\n",
        "lengthsoftrain=[]\n",
        "\n",
        "x,y = dataset.shape\n",
        "min = 100000000000\n",
        "for i in range(x):\n",
        "  lengthsoftrain.append(len(tokenizer(traindata[i])))\n",
        "\n",
        "testdata = testdataset[:,1]\n",
        "testlabel=testdataset[:,0]\n",
        "\n",
        "lengthsoftest=[]\n",
        "x,y = testdataset.shape\n",
        "\n",
        "min = 100000000000\n",
        "for i in range(x):\n",
        "  lengthsoftest.append(len(tokenizer(testdata[i])))"
      ],
      "metadata": {
        "id": "6m043GHNfnGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating Mask: tensor of \"False\" values of size WORDlen base code for the mask from stack overflow\n",
        "\n",
        "WORDlen = 50 #limit on the fixed word length\n",
        "\n",
        "lengthsoftrain=torch.tensor(lengthsoftrain)\n",
        "trainmask=torch.logical_not(torch.arange(WORDlen).expand(len(lengthsoftrain), WORDlen) < lengthsoftrain.unsqueeze(1))\n",
        "\n",
        "lengthsoftest=torch.tensor(lengthsoftest)\n",
        "testmask=torch.logical_not(torch.arange(WORDlen).expand(len(lengthsoftest), WORDlen) < lengthsoftest.unsqueeze(1))"
      ],
      "metadata": {
        "id": "m04cKlA5GmdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Vocabulary\n",
        "#Our implementation of the vocabulary is all words that are seen in training and testing data.\n",
        "\n",
        "def build_vocab(first,second,WORDlen):\n",
        "  L=[]\n",
        "  if first:\n",
        "    for i in traindata:\n",
        "      L.append(tokenizer(i)[:WORDlen])\n",
        "  if second:\n",
        "    for i in testdata:\n",
        "      L.append(tokenizer(i)[:WORDlen])\n",
        "  L=np.array(L)\n",
        "  L=np.concatenate(L)\n",
        "  return list(L)\n",
        "\n",
        "test_vocab_list=dict()\n",
        "train_vocab_list=dict()\n",
        "train_tokens = list(set(build_vocab(True,False,WORDlen)))\n",
        "test_tokens = list(set(build_vocab(False,True,WORDlen)))\n",
        "\n",
        "\"\"\"\n",
        "for wordlength in [50,100]:\n",
        "  z=(len(list(set(build_vocab(True,True,wordlength))))) #a+b+c\n",
        "  x=(len(list(set(build_vocab(True,False,wordlength)))))  #a+c\n",
        "  y=(len(list(set(build_vocab(False,True,wordlength))))) #b+c\n",
        "  c=x+y-z\n",
        "  print(y,y-c)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "for wordlength in [50]:\n",
        "  train_tokens_1 = list(set(build_vocab(True,False,wordlength)))\n",
        "  test_tokens_1 = list(set(build_vocab(False,True,wordlength)))\n",
        "  test_vocab_list = dict(zip(test_tokens_1, range(len(test_tokens_1))))\n",
        "  train_vocab_list = dict(zip(train_tokens_1, range(len(train_tokens_1))))\n",
        "\n",
        "count_test=0\n",
        "count_train=0\n",
        "total=0\n",
        "\n",
        "for word in test_vocab_list:\n",
        "  if not(modelemb.wv.has_index_for(word)): count_test+=1\n",
        "  total+=1\n",
        "  x = modelemb.wv.get_vector(word)\n",
        "\n",
        "\n",
        "  #if word not in train_vocab_list:\n",
        "  #  print(word)\n",
        "for word in train_vocab_list:\n",
        "  if not(modelemb.wv.has_index_for(word)): count_train+=1\n",
        "  total+=1\n",
        "print(count_test/total)\n",
        "print(count_train/total)\n",
        "\n",
        "\"\"\"\n",
        "train_vocab_list=dict(zip(train_tokens, range(len(train_tokens))))\n",
        "test_vocab_list=dict(zip(test_tokens, range(len(test_tokens))))\n",
        "train_vocab_list[\"padding\"]=len(train_tokens)\n",
        "\n",
        "'''\n",
        "vocab_list_one_hot = dict()\n",
        "for i in vocab_list:\n",
        "  length = len(vocab_list)\n",
        "  one_hot = np.zeros(length)\n",
        "  vocab_list[i]\n",
        "  one_hot[vocab_list[i]] = 1\n",
        "  vocab_list_one_hot[i] = one_hot'''\n",
        "\n",
        "newdict=dict()\n",
        "\n",
        "\"\"\"\n",
        "ADDED\n",
        "\"\"\"\n",
        "count=0\n",
        "exists=0\n",
        "for word in test_vocab_list:\n",
        "  count+=1\n",
        "  if modelemb.wv.has_index_for(word):\n",
        "    exists+=1\n",
        "\n",
        "\n",
        "count=0\n",
        "exists=0\n",
        "for word in test_vocab_list:\n",
        "  count+=1\n",
        "  if word in train_vocab_list and modelemb.wv.has_index_for(word):\n",
        "    newdict[word]=train_vocab_list[word]\n",
        "    exists+=1\n"
      ],
      "metadata": {
        "id": "WO590LS1xuV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6641a18-9160-4e77-a214-a61c2500c098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-73ca2dd623f7>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  L=np.array(L)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix=np.zeros((len(newdict),300))\n",
        "\n",
        "l=list(newdict.keys())\n",
        "for wordi in range(len(l)):\n",
        "  matrix[wordi]=modelemb.wv.get_vector(newdict[l[wordi]])\n"
      ],
      "metadata": {
        "id": "8A7HzJXZqay4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NON-SIMILARITY: no similarity version\n",
        "\n",
        "#print(testdata)\n",
        "newtestdata = np.copy(testdata)\n",
        "countreplaced=0\n",
        "totalcount=0\n",
        "\n",
        "for sample_idx in range(len(testdata)):\n",
        "\n",
        "  if sample_idx < 100000:\n",
        "\n",
        "    sample = newtestdata[sample_idx]\n",
        "    print(sample_idx)\n",
        "\n",
        "    tokenized_sample = tokenizer(sample) #tokenized sample\n",
        "\n",
        "    for word_idx in range(len(tokenized_sample)):\n",
        "\n",
        "          word = tokenized_sample[word_idx]\n",
        "          #word = 'apomorphine'\n",
        "          #print(\"word\", word)\n",
        "\n",
        "          if word in train_vocab_list:\n",
        "            continue\n",
        "\n",
        "          else:\n",
        "            tokenized_sample[word_idx] = \"padding\"\n",
        "\n",
        "  else:\n",
        "      break\n",
        "\n",
        "  #now we need to change the tokenized sample back into text\n",
        "  new_sample = \" \".join(tokenized_sample)\n",
        "  newtestdata[sample_idx] = new_sample\n",
        "\n",
        "testdata = newtestdata\n",
        "\n",
        "#np.savetxt('/content/drive/MyDrive/Deep Learning Project 2/test_w_similar_NON.txt', testdata, fmt='%s',  delimiter='\\t')\n",
        "np.savetxt('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/test_w_similar_NON.txt', testdata, fmt='%s',  delimiter='\\t')\n",
        "\n",
        "#newdat_NON = np.loadtxt('/content/drive/MyDrive/Deep Learning Project 2/test_w_similar_NON.txt', dtype=str, delimiter='\\t')\n",
        "newdat_NON = np.loadtxt('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/test_w_similar_NON.txt', dtype=str, delimiter='\\t')\n"
      ],
      "metadata": {
        "id": "O-y7gCNh2Att"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SIMILARITY: Replace words in test data with most similar word from the training data using similarity\n",
        "\n",
        "newtestdata = np.copy(testdata)\n",
        "countreplaced=0\n",
        "totalcount=0\n",
        "\n",
        "for sample_idx in range(len(testdata)): #lymphocyte ajkfkj. jakjfks\n",
        "\n",
        "\n",
        "  if sample_idx < 10000:\n",
        "\n",
        "    sample = newtestdata[sample_idx]\n",
        "    tokenized_sample = tokenizer(sample) #tokenized sample\n",
        "\n",
        "    for word_idx in range(len(tokenized_sample)):\n",
        "\n",
        "          changed=False\n",
        "          word = tokenized_sample[word_idx]\n",
        "\n",
        "          if word in train_vocab_list:\n",
        "            continue\n",
        "\n",
        "          totalcount+=1\n",
        "\n",
        "          if (word not in train_vocab_list) and (not modelemb.wv.has_index_for(word)):\n",
        "            tokenized_sample[word_idx] = \"padding\"\n",
        "            continue\n",
        "\n",
        "          \"\"\"\n",
        "          matmulti = np.matmul(matrix, modelemb.wv.get_vector(word))\n",
        "          closest_id = np.argmax(matmulti)\n",
        "          for topi in np.argsort(matmulti)[-10:]:\n",
        "            print(l[topi])\n",
        "          print(word,\"   i\")\n",
        "          closestword = l[closest_id]\n",
        "\n",
        "          tokenized_sample[word_idx] = closestword\n",
        "          countreplaced+=1\n",
        "\n",
        "          #print(closestword,word)\n",
        "          \"\"\"\n",
        "\n",
        "          for simword, prob in modelemb.wv.most_similar(word): #lympocute .....\n",
        "\n",
        "            if simword in train_vocab_list:\n",
        "              tokenized_sample[word_idx] = simword #lymphotiocjajskma --> lympocu\n",
        "              changed=True\n",
        "              countreplaced+=1\n",
        "              continue\n",
        "\n",
        "\n",
        "          if changed == False:\n",
        "            #print(\"here 3\")\n",
        "            tokenized_sample[word_idx] = \"padding\"\n",
        "\n",
        "          \"\"\"\n",
        "          # WORDS that have an embedding = 85% (train test)\n",
        "          SIMILARITY SELF LEARNED:\n",
        "          - MATRIX: 70% of vocab words, 30% need be replaced: .7+.7*.3 = .91 there needs an medical embedding for that test word\n",
        "          - PRE-BUILT FCN: 70%, 30%: .7+.37*.3 = .79\n",
        "         \"\"\"\n",
        "\n",
        "  else:\n",
        "      break\n",
        "\n",
        "  #now we need to change the tokenized sample back into text\n",
        "  new_sample = \" \".join(tokenized_sample)\n",
        "  newtestdata[sample_idx] = new_sample\n",
        "\n",
        "testdata = newtestdata\n",
        "\n",
        "np.savetxt('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/test_w_similar_newtokenizer.txt', testdata, fmt='%s',  delimiter='\\t')\n",
        "#np.savetxt('/content/drive/MyDrive/Deep Learning Project 2/test_w_similar_newtokenizer.txt', testdata, fmt='%s',  delimiter='\\t')\n",
        "newdat = np.loadtxt('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/test_w_similar_newtokenizer.txt', dtype=str, delimiter='\\t')\n",
        "#newdat = np.loadtxt('/content/drive/MyDrive/Deep Learning Project 2/test_w_similar_newtokenizer.txt', dtype=str, delimiter='\\t')\n"
      ],
      "metadata": {
        "id": "ZGEc2PhOQbvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update from prev block: NON-Similarity\n",
        "testdata = newdat_NON"
      ],
      "metadata": {
        "id": "p6q0axRp3Ns2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update from prev block: Similarity\n",
        "testdata = newdat"
      ],
      "metadata": {
        "id": "VMyy8LwGb40N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "count=0\n",
        "total=0\n",
        "for word in test_vocab_list:\n",
        "  if (glove_vectors.has_index_for(word)) and not(modelemb.wv.has_index_for('the')):\n",
        "    count+=1\n",
        "  total+=1\n",
        "print(count/total)\n",
        "\n",
        "modelemb.wv.similarity('copd', 'chronic_obstructive_pulmonary_disease')\n",
        "print(glove_vectors.most_similar('lymphocyte'))\n",
        "\n",
        "\"\"\"\n",
        "for word,_ in modelemb.wv.most_similar('apomorphine'):\n",
        "  print(word in train_vocab_list)"
      ],
      "metadata": {
        "id": "K37hjaE6Nzam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "#tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, txt, labels,mask):\n",
        "        self.labels = labels\n",
        "        self.text = txt\n",
        "        self.mask=mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]-1\n",
        "        text = self.text[idx]\n",
        "\n",
        "        #print(text)\n",
        "        text = tokenizer(text)\n",
        "\n",
        "        text = np.array(text)\n",
        "\n",
        "        #while np.shape(text)[0] < WORDlen:\n",
        "            #text = np.append(text, text)\n",
        "\n",
        "        text = text[:WORDlen]\n",
        "\n",
        "        while np.shape(text)[0] < WORDlen:\n",
        "            text = np.append(text, [\"padding\"])\n",
        "\n",
        "        text = list(text)\n",
        "        mask=self.mask[idx]\n",
        "\n",
        "        '''\n",
        "        for wordi in range(len(text)):\n",
        "          changed=False\n",
        "          word=text[wordi]\n",
        "          #print(word,word in train_vocab_list)\n",
        "\n",
        "          if word in train_vocab_list:\n",
        "\n",
        "            continue\n",
        "\n",
        "          if (word not in train_vocab_list) and (not glove_vectors.has_index_for(word)):\n",
        "            text[wordi]=\"padding\"\n",
        "            continue\n",
        "\n",
        "          for simword,prob in glove_vectors.most_similar(word): #lympocute .....\n",
        "            if simword in train_vocab_list:\n",
        "              text[wordi]=simword #lymphotiocjajskma --> lympocu\n",
        "              changed=True\n",
        "              continue\n",
        "          if changed==False:\n",
        "            text[wordi]=\"padding\"\n",
        "        #print(text)\n",
        "        '''\n",
        "\n",
        "        \"\"\"\n",
        "          word=text[wordi]\n",
        "          if word not in train_vocab_list:\n",
        "            # text[wordi]=\"padding\"\n",
        "            if not modelemb.wv.has_index_for(word):\n",
        "              text[wordi]=\"padding\"\n",
        "            else:\n",
        "              closestword=None\n",
        "              threshold=-1000\n",
        "              for trainword in train_vocab_list:\n",
        "                if modelemb.wv.has_index_for(trainword):\n",
        "                  trem=modelemb.wv.get_vector(trainword)\n",
        "                  wordem=modelemb.wv.get_vector(word)\n",
        "                  cosine_similarity = np.dot(trem,wordem)/(np.linalg.norm(trem)* np.linalg.norm(wordem))\n",
        "                  if cosine_similarity>threshold:\n",
        "                    closestword=trainword\n",
        "                    threshold=cosine_similarity\n",
        "              text[wordi]=closestword\n",
        "          \"\"\"\n",
        "        sample = {\"Text\": text, \"Class\": label, \"Mask\": mask}\n",
        "        return sample\n",
        "\n",
        "traindata_set = CustomTextDataset(traindata, trainlabel,trainmask)\n",
        "testdata_set = CustomTextDataset(testdata, testlabel,testmask) #CHANGED\n"
      ],
      "metadata": {
        "id": "XFPiE3MFi5po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ac48e9-d39d-4276-e10f-0e7e757b7af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2888, 50])\n",
            "{'Text': ['treatment', 'of', 'childhood', 'padding', 'diseases', 'with', 'recombinant', 'interferon', 'alfa', '2a', '.', 'a', 'heterogeneous', 'group', 'of', 'five', 'patients', 'with', 'progressive', ',', 'invasive', 'padding', 'diseases', 'including', 'pulmonary', 'hemangiomatosis', ',', 'angiosarcoma', ',', 'or', 'massive', 'hemangioma', 'with', 'associated', 'consumptive', 'coagulopathy', 'were', 'treated', 'with', 'interferon', 'alfa', '2a', 'for', 'periods', 'of', '17', 'to', '33', 'months', '.'], 'Class': 0, 'Mask': tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Load --> getting shuffled batches for training\n",
        "\n",
        "trainloader = DataLoader(traindata_set, batch_size = 64, shuffle = True)\n",
        "trainloader2 = DataLoader(traindata_set, batch_size = 4000, shuffle = True)\n",
        "testloader2 = DataLoader(testdata_set, batch_size = 2000, shuffle = True)\n",
        "\n",
        "#parameters\n",
        "num_epochs = 50\n",
        "\n",
        "#for epochs in range(num_epochs):\n",
        "\"\"\"\n",
        "for i in trainloader:\n",
        "  print(i)\n",
        "\n",
        "for idx,D in enumerate(trainloader):\n",
        "    print(D)\n",
        "    break\n",
        "\"\"\"\n",
        "\n",
        "for (idx, batch) in enumerate(testloader2):\n",
        "    # Print the 'text' data of the batch\n",
        "    print(np.array(batch['Text']).T[0])\n",
        "    print(idx, 'Text data: ', (batch['Text'])[0][1])\n",
        "    # Print the 'class' data of batch\n",
        "    print(idx, 'Class data: ', len(batch['Class']), '\\n')\n",
        "\n",
        "    break\n"
      ],
      "metadata": {
        "id": "aHnZgU3Nlvfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer Implementation\n",
        "#lambda trick and checkpoint from other students in OH\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "dropout_ratio=0.3\n",
        "\n",
        "class MyNeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNeuralNet,self).__init__()\n",
        "        self.embedding=nn.Embedding(int(len(train_vocab_list)+1),300)\n",
        "        self.positional_encoding = PositionalEncoding(d_model=300, dropout=0)\n",
        "\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=300, nhead=1, batch_first=True,dropout=dropout_ratio)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, 1)\n",
        "\n",
        "\n",
        "        self.flatten=nn.Flatten(1,2) #N, L, E\n",
        "        inp=300\n",
        "\n",
        "        self.dropout = nn.Dropout(.2)\n",
        "        self.hid_layer = nn.Linear(inp*WORDlen, 1000)\n",
        "        self.dropout2 = nn.Dropout(.3)\n",
        "        out=inp*WORDlen\n",
        "        self.out_layer = nn.Linear(out, 5)\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "\n",
        "        #x, attn_output_weights=self.multihead_attn(x,x,x)\n",
        "        x=self.embedding(x)\n",
        "        self.dummy_tensor = torch.ones(1, dtype=torch.float32, requires_grad=True)\n",
        "        x=self.positional_encoding(x)\n",
        "        x=torch.utils.checkpoint.checkpoint(lambda x,y: self.transformer_encoder(x,src_key_padding_mask=mask),x, self.dummy_tensor)\n",
        "        x=self.flatten(x)\n",
        "        x=self.dropout(x)\n",
        "        #x=self.hid_layer(x)\n",
        "        #x=self.dropout2(x)\n",
        "        #attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "        x=self.out_layer(x)\n",
        "        yhat=self.sm(x)\n",
        "\n",
        "        #N 27 128  switch axes 27 N 128 --> 0 N 128\n",
        "\n",
        "        return x, yhat\n"
      ],
      "metadata": {
        "id": "pfagAcH7v4x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Positional Encoding base from Pytorch website\n",
        "\n",
        "from torch import nn, Tensor\n",
        "import math\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "\n",
        "        x=torch.swapaxes(x,0,1)\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        x=self.dropout(x)\n",
        "        x=torch.swapaxes(x,0,1)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "2-ipVGmUX-sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Training Data w Embeddings\n",
        "#After we have tokenized and shuffled batches of our training data, we need to prepare data for training in the LSTM,\n",
        "#by converting each word in the tokenized text to a 300d embedding.\n",
        "#We use the Clinical Embeddings here: https://github.com/gweissman/clinical_embeddings/blob/master/README.md.\n",
        "\n",
        "def prepareXY(batchsize, batch,test=False):\n",
        "        x = torch.from_numpy(np.zeros((batchsize, WORDlen)))\n",
        "        x2=np.array(batch['Text']).T\n",
        "        mask=batch['Mask']\n",
        "\n",
        "        for i2 in range(batchsize):\n",
        "          if test: print(i2)\n",
        "          for j2 in range(WORDlen): #27 --> 206\n",
        "            if test: print(x2[i2])\n",
        "\n",
        "            x[i2,j2] = train_vocab_list[x2[i2][j2]] #\"the\"->50\n",
        "\n",
        "\n",
        "\n",
        "        x = torch.tensor(x).to(torch.int64)\n",
        "        #x = torch.tensor(x).to(torch.float32)\n",
        "        y = np.array(batch['Class'])\n",
        "        y = torch.from_numpy(y.astype(int))\n",
        "\n",
        "        yactual = []\n",
        "        for j in range(batchsize):\n",
        "          one_hot = np.zeros(5)\n",
        "          one_hot[y[j]] = 1\n",
        "          yactual.append(one_hot)\n",
        "\n",
        "\n",
        "        yactual = torch.Tensor(yactual)\n",
        "        return x,yactual,mask\n",
        "\n",
        "def accuracyfun(batchsize,q,yactual,include=False,batch=None):\n",
        "        accuracy=0\n",
        "        for i in range(batchsize):\n",
        "          if torch.argmax(q[i]) == torch.argmax(yactual[i]):\n",
        "            accuracy+=1\n",
        "          else:\n",
        "            if include:\n",
        "              print(\"exp\",\"true\",torch.argmax(q[i]),torch.argmax(yactual[i]))\n",
        "              print(batch[i])\n",
        "\n",
        "        accuracy/=batchsize\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "O7MG6Lg8XrEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchucheckpoint"
      ],
      "metadata": {
        "id": "eRH-I6ouyfgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer Training Loop\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model = MyNeuralNet()\n",
        "\n",
        "#model.load_state_dict(torch.load('transmodel2.pkl'))\n",
        "learning_rate = 10**-1\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "#optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "trainlosses,testlosses,testaccuracies,trainaccuracies = [],[],[],[]\n",
        "#model.to(device)\n",
        "\n",
        "for epoch in range(40):\n",
        "\n",
        "\n",
        "\n",
        "    for (idx, batch) in enumerate(trainloader):\n",
        "    # Print the 'text' data of the batch\n",
        "\n",
        "    # Print the 'class' data of batch\n",
        "        if idx>80: break #WAS 80\n",
        "        x,yactual,mask = prepareXY(64,batch)\n",
        "        mask=(x == train_vocab_list[\"padding\"])\n",
        "\n",
        "        \"\"\"\n",
        "        mask=torch.zeros((64,WORDlen))\n",
        "        for ix in range(len(x)):\n",
        "          row=x[ix]\n",
        "          for elem in range(len(row)):\n",
        "            if torch.equal(row[elem],torch.zeros(300)):\n",
        "              mask[ix,elem]=1\n",
        "        \"\"\"\n",
        "\n",
        "        p,q=model(x,mask)\n",
        "        loss =loss_fn(p,yactual)\n",
        "\n",
        "        accuracy = accuracyfun(64,q,yactual)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    for idx, batch in enumerate(trainloader2):\n",
        "\n",
        "\n",
        "        x,yactual,mask = prepareXY(4000,batch)\n",
        "        mask=(x == train_vocab_list[\"padding\"])\n",
        "        p,q=model(x,mask)\n",
        "        loss =loss_fn(p,yactual)\n",
        "\n",
        "        accuracy = accuracyfun(4000,q,yactual)\n",
        "        counts=[0,0,0,0,0]\n",
        "\n",
        "        for i in range(4000):\n",
        "          counts[torch.argmax(q[i])-1]+=1\n",
        "\n",
        "\n",
        "        print(counts)\n",
        "        break\n",
        "    for idx, batch in enumerate(testloader2):\n",
        "        x,yactual,mask = prepareXY(2000,batch)\n",
        "        mask=(x == train_vocab_list[\"padding\"])\n",
        "        p,q=model(x,mask)\n",
        "        testloss =loss_fn(p,yactual)\n",
        "\n",
        "        testaccuracy=accuracyfun(2000,q,yactual)\n",
        "\n",
        "        break\n",
        "\n",
        "    torch.save(model.state_dict(), 'transmodel2.pkl')\n",
        "    trainlosses.append(loss.item())\n",
        "    testlosses.append(testloss.item())\n",
        "    testaccuracies.append(testaccuracy)\n",
        "    trainaccuracies.append(accuracy)\n",
        "    print(loss.item(),accuracy,testloss.item(),testaccuracy)\n",
        "    scheduler.step()\n",
        "\n",
        "print(trainlosses,testlosses)\n",
        "print(testaccuracies,trainaccuracies)\n"
      ],
      "metadata": {
        "id": "qb3GvbysEmyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyNeuralNet()\n",
        "model.load_state_dict(torch.load('/content/transmodel2.pkl'))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "#cm = confusion_matrix(y_true, y_pred)\n",
        "#print(cm)\n",
        "\n",
        "for idx, batch in enumerate(testloader2):\n",
        "        size=2000\n",
        "        x,yactual,mask = prepareXY(size,batch)\n",
        "        mask=(x == train_vocab_list[\"padding\"])\n",
        "        p,q=model(x,mask)\n",
        "        testloss =loss_fn(p,yactual)\n",
        "\n",
        "        testaccuracy=accuracyfun(size,q,yactual)\n",
        "        print(testaccuracy)\n",
        "        truelabs=yactual\n",
        "        L=[]\n",
        "        for i in range(size):\n",
        "          L.append(torch.argmax(q[i]))\n",
        "\n",
        "        L=np.array(L,dtype=int)\n",
        "\n",
        "        print(yactual.shape)\n",
        "        print(type(L[0]),type(np.array(yactual,dtype=int)[0]))\n",
        "        print(confusion_matrix(np.array(torch.argmax(yactual,axis=1)), L))\n",
        "        print(confusion_matrix(np.array(torch.argmax(yactual,axis=1)), L))\n",
        "        testaccuracy=accuracyfun(size,q,yactual,True,np.array(batch['Text']).T)\n",
        "\n",
        "\n",
        "        break"
      ],
      "metadata": {
        "id": "MPv2iT4FYt3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}