{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gensim\n",
        "pip install torch\n",
        "pip install torch.utils"
      ],
      "metadata": {
        "id": "CNcYr0zK59JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.checkpoint\n",
        "import gensim.models"
      ],
      "metadata": {
        "id": "ro8vrqM_wftV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing and Modifying Training & Testing Data\n",
        "\n",
        "\n",
        "#Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Modules\n",
        "import gensim.models\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
        "\n",
        "#Deepti Import Data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/medical_tc_train.csv')\n",
        "testdataset = pd.read_csv('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/medical_tc_test.csv')\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "modelemb = Word2Vec.load('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/w2v_OA_CR_300d.bin')\n",
        "\n",
        "dataset = pd.DataFrame.to_numpy(dataset)\n",
        "testdataset = pd.DataFrame.to_numpy(testdataset)\n",
        "\n",
        "#In training dataset, make each class have 1100 abstract samples to even out classes\n",
        "dataset = dataset[dataset[:, 0].argsort()]\n",
        "datasetnew=np.copy(dataset[:5500,:])\n",
        "\n",
        "Dofstarts=dict()\n",
        "for i in range(len(dataset)):\n",
        "  row=dataset[i]\n",
        "  if int(row[0]) not in Dofstarts:\n",
        "    Dofstarts[int(row[0])] = i\n",
        "\n",
        "for i in range(5):\n",
        "  i = i+1\n",
        "  valtostart=Dofstarts[i]\n",
        "  datasetnew[(i-1)*1100:i*1100,:] = dataset[valtostart:valtostart+1100,:]\n",
        "dataset=datasetnew"
      ],
      "metadata": {
        "id": "qJFB_ivlM7Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing and Modifying Training & Testing Data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#modules\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
        "\n",
        "#Deepti Import Data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Deep Learning Project 2/medical_tc_train.csv')\n",
        "testdataset = pd.read_csv('/content/drive/MyDrive/Deep Learning Project 2/medical_tc_test.csv')\n",
        "#wordembeddings = pd.read_csv('/content/drive/MyDrive/Deep Learning Project 2/w2v_100d_oa_all.tar.gz')\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "modelemb = Word2Vec.load('/content/drive/MyDrive/Deep Learning Project 2/w2v_OA_CR_300d.bin')\n",
        "modelemb.wv.has_index_for('diabetes')\n",
        "\n",
        "dataset = pd.DataFrame.to_numpy(dataset)\n",
        "testdataset = pd.DataFrame.to_numpy(testdataset)\n",
        "\n",
        "#In training dataset, make each class have 1100 abstract samples to even out classes\n",
        "dataset = dataset[dataset[:, 0].argsort()]\n",
        "datasetnew=np.copy(dataset[:5500,:])\n",
        "\n",
        "Dofstarts=dict()\n",
        "for i in range(len(dataset)):\n",
        "  row=dataset[i]\n",
        "  if int(row[0]) not in Dofstarts:\n",
        "    Dofstarts[int(row[0])] = i\n",
        "\n",
        "for i in range(5):\n",
        "  i = i+1\n",
        "  valtostart=Dofstarts[i]\n",
        "  datasetnew[(i-1)*1100:i*1100,:] = dataset[valtostart:valtostart+1100,:]\n",
        "dataset=datasetnew"
      ],
      "metadata": {
        "id": "Nb2sgmYuhGMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Tokenization\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "########## ADDED\n",
        "\n",
        "import spacy\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Create a custom tokenizer using spacy\n",
        "basic_english_tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Define a custom tokenizer that also splits on dashes\n",
        "def custom_tokenizer(text):\n",
        "    # Use the basic_english tokenizer and split on dashes\n",
        "    tokens = basic_english_tokenizer(text)\n",
        "    # Flatten the list of tokens and split on dashes\n",
        "    tokens = [subtoken for token in tokens for subtoken in token.split(\"-\")]\n",
        "    return tokens\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a test-sentence with a dash.\"\n",
        "tokens = custom_tokenizer(text)\n",
        "tokenizer=custom_tokenizer\n",
        "\n",
        "##############\n",
        "\n",
        "traindata = dataset[:,1]\n",
        "trainlabel=dataset[:,0]\n",
        "\n",
        "#get the lengths of the training data abstracts\n",
        "lengthsoftrain=[]\n",
        "x,y = dataset.shape\n",
        "min = 100000000000\n",
        "\n",
        "for i in range(x):\n",
        "  lengthsoftrain.append(len(tokenizer(traindata[i])))\n",
        "\n",
        "testdata = testdataset[:,1]\n",
        "testlabel=testdataset[:,0]\n",
        "\n",
        "#get the lengths of the testing data abstracts\n",
        "lengthsoftest=[]\n",
        "x,y = testdataset.shape\n",
        "\n",
        "old=get_tokenizer('basic_english')\n",
        "min = 100000000000\n",
        "for i in range(x):\n",
        "  lengthsoftest.append(len(tokenizer(testdata[i])))\n"
      ],
      "metadata": {
        "id": "6m043GHNfnGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating Mask: tensor of \"False\" values of size WORDlen - base code for mask from stack overflow\n",
        "\n",
        "WORDlen = 50 #In LSTM, we used WORDlen = 20\n",
        "\n",
        "lengthsoftrain = torch.tensor(lengthsoftrain)\n",
        "trainmask = torch.logical_not(torch.arange(WORDlen).expand(len(lengthsoftrain), WORDlen) < lengthsoftrain.unsqueeze(1))\n",
        "\n",
        "lengthsoftest=torch.tensor(lengthsoftest)\n",
        "testmask= torch.logical_not(torch.arange(WORDlen).expand(len(lengthsoftest), WORDlen) < lengthsoftest.unsqueeze(1))\n"
      ],
      "metadata": {
        "id": "m04cKlA5GmdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing Training & Testing Data\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "#tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, txt, labels,mask):\n",
        "        self.labels = labels\n",
        "        self.text = txt\n",
        "        self.mask=mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]-1\n",
        "        text = self.text[idx]\n",
        "\n",
        "        text = tokenizer(text)\n",
        "        text = np.array(text)\n",
        "\n",
        "        #while np.shape(text)[0] < WORDlen:\n",
        "            #text = np.append(text, text)\n",
        "\n",
        "        text = text[:WORDlen] #WORDlen = 50\n",
        "\n",
        "        while np.shape(text)[0] < WORDlen:\n",
        "            text = np.append(text, [\"hjgjhgjhgfgdgfdgfdfjhj\"])\n",
        "\n",
        "        text = list(text)\n",
        "        mask = self.mask[idx] #masking the abtract text\n",
        "\n",
        "        for wordi in range(len(text)):\n",
        "          word = text[wordi]\n",
        "          if not modelemb.wv.has_index_for(word): #if no clinical embedding\n",
        "            mask[wordi] = True #we mask these words\n",
        "\n",
        "        sample = {\"Text\": text, \"Class\": label, \"Mask\": mask}\n",
        "        return sample\n",
        "\n",
        "#Get tokenized versions of the abstract samples in the training and testing where words without medical embeddings are masked\n",
        "traindata_set = CustomTextDataset(traindata, trainlabel, trainmask)\n",
        "testdata_set = CustomTextDataset(testdata, testlabel, testmask) #random think into it\n"
      ],
      "metadata": {
        "id": "XFPiE3MFi5po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Load --> getting shuffled batches for training\n",
        "\n",
        "trainloader = DataLoader(traindata_set, batch_size = 64, shuffle = True)\n",
        "trainloader2 = DataLoader(traindata_set, batch_size = 2000, shuffle = True) #WAS 4000\n",
        "testloader2 = DataLoader(testdata_set, batch_size = 2000, shuffle = True)\n",
        "\n",
        "#parameters\n",
        "num_epochs = 50\n",
        "\n",
        "#for epochs in range(num_epochs):\n",
        "\"\"\"\n",
        "for i in trainloader:\n",
        "  print(i)\n",
        "\n",
        "for idx,D in enumerate(trainloader):\n",
        "    print(D)\n",
        "    break\n",
        "\"\"\"\n",
        "\n",
        "for (idx, batch) in enumerate(trainloader):\n",
        "    # Print the 'text' data of the batch\n",
        "    print(np.array(batch['Text']).T[0])\n",
        "    print(idx, 'Text data: ', (batch['Text'])[0][1])\n",
        "    # Print the 'class' data of batch\n",
        "    print(idx, 'Class data: ', len(batch['Class']), '\\n')\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "aHnZgU3Nlvfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Vocabulary\n",
        "#Our current implementation of the vocabulary is all words that are seen in training and testing data.\n",
        "\n",
        "def build_vocab(first, second, WORDlen): #WORDlen = 50 (from above)\n",
        "  L = []\n",
        "\n",
        "  if first: #words from train\n",
        "    for i in traindata:\n",
        "      L.append(tokenizer(i)[:WORDlen]) #get cropped tokenized abstract\n",
        "\n",
        "  if second: #words from test\n",
        "    for i in testdata:\n",
        "      L.append(tokenizer(i)[:WORDlen]) #get cropped tokenized abstract\n",
        "\n",
        "  L=np.array(L)\n",
        "  L=np.concatenate(L)\n",
        "\n",
        "  return list(L)\n",
        "\n",
        "test_vocab_list=dict()\n",
        "train_vocab_list=dict()\n",
        "train_tokens = list(set(build_vocab(True, True, WORDlen)))\n",
        "\n",
        "\"\"\"\n",
        "for wordlength in [50,100]:\n",
        "  z=(len(list(set(build_vocab(True,True,wordlength))))) #a+b+c\n",
        "  x=(len(list(set(build_vocab(True,False,wordlength)))))  #a+c\n",
        "  y=(len(list(set(build_vocab(False,True,wordlength))))) #b+c\n",
        "  c=x+y-z\n",
        "  print(y,y-c)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "for wordlength in [50]:\n",
        "  train_tokens_1 = list(set(build_vocab(True,False,wordlength)))\n",
        "  test_tokens_1 = list(set(build_vocab(False,True,wordlength)))\n",
        "  test_vocab_list = dict(zip(test_tokens_1, range(len(test_tokens_1))))\n",
        "  train_vocab_list = dict(zip(train_tokens_1, range(len(train_tokens_1))))\n",
        "\n",
        "count_test=0\n",
        "count_train=0\n",
        "total=0\n",
        "\n",
        "for word in test_vocab_list:\n",
        "  if not(modelemb.wv.has_index_for(word)): count_test+=1\n",
        "  total+=1\n",
        "  x = modelemb.wv.get_vector(word)\n",
        "\n",
        "\n",
        "  #if word not in train_vocab_list:\n",
        "  #  print(word)\n",
        "for word in train_vocab_list:\n",
        "  if not(modelemb.wv.has_index_for(word)): count_train+=1\n",
        "  total+=1\n",
        "print(count_test/total)\n",
        "print(count_train/total)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "vocab_list = dict(zip(train_tokens, range(len(train_tokens))))\n",
        "\n",
        "'''\n",
        "vocab_list_one_hot = dict()\n",
        "for i in vocab_list:\n",
        "  length = len(vocab_list)\n",
        "  one_hot = np.zeros(length)\n",
        "  vocab_list[i]\n",
        "  one_hot[vocab_list[i]] = 1\n",
        "  vocab_list_one_hot[i] = one_hot'''\n"
      ],
      "metadata": {
        "id": "WO590LS1xuV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer Implementation\n",
        "#lambda trick and checkpoint from other students in OH\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "dropout_ratio=0.3\n",
        "\n",
        "class MyNeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNeuralNet,self).__init__()\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(d_model=300, dropout=0) #300 dimensional medical embeddings\n",
        "\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=300, nhead=1, batch_first=True, dropout=dropout_ratio)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, 1)\n",
        "\n",
        "        #self.multihead_attn = nn.MultiheadAttention(embed_dim=300, num_heads=2)\n",
        "        #self.transformer =nn.Transformer(d_model=300,nhead=2,batch_first=True)\n",
        "        #nn.transformer\n",
        "        #self.rnn = torch.nn.LSTM(300,100,1,batch_first=True,dropout=.2)\n",
        "        self.flatten=nn.Flatten(1,2) #N, L, E\n",
        "        inp=300\n",
        "\n",
        "        self.dropout = nn.Dropout(.3)\n",
        "        self.hid_layer = nn.Linear(inp*WORDlen, 1000)\n",
        "        self.dropout2 = nn.Dropout(.3)\n",
        "        out=inp*WORDlen\n",
        "        #out=9000\n",
        "        self.out_layer = nn.Linear(out, 5)\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "\n",
        "        #x, attn_output_weights=self.multihead_attn(x,x,x)\n",
        "        self.dummy_tensor = torch.ones(1, dtype=torch.float32, requires_grad=True)\n",
        "        x=self.positional_encoding(x)\n",
        "        x=torch.utils.checkpoint.checkpoint(lambda x,y: self.transformer_encoder(x,src_key_padding_mask=mask),x, self.dummy_tensor)\n",
        "        x=self.flatten(x)\n",
        "        x=self.dropout(x)\n",
        "        #x=self.hid_layer(x)\n",
        "        #x=self.dropout2(x)\n",
        "        #attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "        x=self.out_layer(x)\n",
        "        yhat=self.sm(x)\n",
        "\n",
        "        #N 27 128  switch axes 27 N 128 --> 0 N 128\n",
        "\n",
        "        return x, yhat"
      ],
      "metadata": {
        "id": "pfagAcH7v4x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add A Positional Encoding modified from torch website\n",
        "\n",
        "from torch import nn, Tensor\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x=torch.swapaxes(x,0,1)\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        x=self.dropout(x)\n",
        "        x=torch.swapaxes(x,0,1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2-ipVGmUX-sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Training Data w Embeddings\n",
        "#After we have tokenized and shuffled batches of our training data, we need to prepare data for training in the LSTM,\n",
        "#by converting each word in the tokenized text to a 300d embedding.\n",
        "#We use the Clinical Embeddings here: https://github.com/gweissman/clinical_embeddings/blob/master/README.md.\n",
        "\n",
        "def prepareXY(batchsize,batch):\n",
        "        #x = torch.from_numpy(np.zeros((batchsize,WORDlen)))\n",
        "        #x = torch.from_numpy(np.zeros((batchsize,WORDlen,300)))\n",
        "\n",
        "        x=(-2)*torch.rand((batchsize,WORDlen,300))+1\n",
        "\n",
        "        x2=np.array(batch['Text']).T\n",
        "        mask=batch['Mask']\n",
        "\n",
        "        for i2 in range(batchsize):\n",
        "          for j2 in range(WORDlen): #27 --> 206\n",
        "\n",
        "            #x[i2,j2] = vocab_list[x2[i2][j2]] #\"the\"->50\n",
        "            word=x2[i2][j2]\n",
        "            if modelemb.wv.has_index_for(word):\n",
        "              x[i2,j2]=torch.from_numpy(modelemb.wv.get_vector(word))\n",
        "\n",
        "            else:\n",
        "              #x[i2,j2]=torch.zeros(300)\n",
        "              x[i2,j2]=(-2)*torch.rand(300)+1 #SEARCH dict for closest word and add its embedding word2vec embedding -> take dot product with all the other words to find most similar word -> use its word embedding\n",
        "\n",
        "\n",
        "        #x = torch.tensor(x).to(torch.int64)\n",
        "        x = torch.tensor(x).to(torch.float32)\n",
        "        y = np.array(batch['Class'])\n",
        "        y = torch.from_numpy(y.astype(int))\n",
        "\n",
        "        yactual = []\n",
        "        for j in range(batchsize):\n",
        "          one_hot = np.zeros(5)\n",
        "          one_hot[y[j]] = 1\n",
        "          yactual.append(one_hot)\n",
        "\n",
        "\n",
        "        yactual = torch.Tensor(yactual)\n",
        "        return x,yactual,mask\n",
        "\n",
        "#Get model accuracy\n",
        "def accuracyfun(batchsize,q,yactual,include=False,batch=None):\n",
        "        accuracy=0\n",
        "        for i in range(batchsize):\n",
        "          if torch.argmax(q[i]) == torch.argmax(yactual[i]):\n",
        "            accuracy+=1\n",
        "          else:\n",
        "            if include:\n",
        "              print(\"exp\",\"true\",torch.argmax(q[i]),torch.argmax(yactual[i]))\n",
        "              print(batch[i])\n",
        "\n",
        "        accuracy/=batchsize\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "O7MG6Lg8XrEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer Training Loop\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model = MyNeuralNet()\n",
        "\n",
        "#model.load_state_dict(torch.load('transmodel.pkl'))\n",
        "learning_rate = 1e-3\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "#optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "trainlosses,testlosses,testaccuracies,trainaccuracies = [],[],[],[]\n",
        "\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "#model.to(device)\n",
        "\n",
        "for epoch in range(40):\n",
        "    for (idx, batch) in enumerate(trainloader):\n",
        "    # Print the 'text' data of the batch\n",
        "\n",
        "    # Print the 'class' data of batch\n",
        "        if idx>80: break\n",
        "        x,yactual,mask = prepareXY(64,batch)\n",
        "\n",
        "        \"\"\"\n",
        "        mask=torch.zeros((64,WORDlen))\n",
        "\n",
        "        for ix in range(len(x)):\n",
        "          row=x[ix]\n",
        "          for elem in range(len(row)):\n",
        "            if torch.equal(row[elem],torch.zeros(300)):\n",
        "              mask[ix,elem]=1\n",
        "        print(mask)\n",
        "        \"\"\"\n",
        "\n",
        "        p,q=model(x,mask)\n",
        "\n",
        "        loss =loss_fn(p,yactual)\n",
        "        accuracy = accuracyfun(64,q,yactual)\n",
        "\n",
        "        #print(model.state_dict())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    for idx, batch in enumerate(trainloader2):\n",
        "\n",
        "\n",
        "        x,yactual,mask = prepareXY(2000,batch)\n",
        "        p,q=model(x,mask)\n",
        "        loss =loss_fn(p,yactual)\n",
        "\n",
        "        accuracy = accuracyfun(2000,q,yactual)\n",
        "        counts=[0,0,0,0,0]\n",
        "\n",
        "        for i in range(2000):\n",
        "          counts[torch.argmax(q[i])-1]+=1\n",
        "\n",
        "\n",
        "        print(counts)\n",
        "        break\n",
        "\n",
        "    for idx, batch in enumerate(testloader2):\n",
        "        x,yactual,mask = prepareXY(2000,batch)\n",
        "        p,q=model(x,mask)\n",
        "        testloss =loss_fn(p,yactual)\n",
        "\n",
        "        testaccuracy=accuracyfun(2000,q,yactual)\n",
        "\n",
        "        break\n",
        "\n",
        "    torch.save(model.state_dict(), 'transmodelnewtokenizer50-2.pkl')\n",
        "    trainlosses.append(loss.item())\n",
        "    testlosses.append(testloss.item())\n",
        "    testaccuracies.append(testaccuracy)\n",
        "    trainaccuracies.append(accuracy)\n",
        "    print(loss.item(),accuracy,testloss.item(),testaccuracy)\n",
        "    #scheduler.step()\n",
        "\n",
        "print(trainlosses,testlosses)\n",
        "print(testaccuracies,trainaccuracies)\n"
      ],
      "metadata": {
        "id": "qb3GvbysEmyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = MyNeuralNet()\n",
        "#model.load_state_dict(torch.load('/content/transmodelnewtokenizer50-2.pkl'))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "#cm = confusion_matrix(y_true, y_pred)\n",
        "#print(cm)\n",
        "\n",
        "for idx, batch in enumerate(testloader2):\n",
        "        size=2000\n",
        "        x,yactual,mask = prepareXY(size,batch)\n",
        "\n",
        "        p,q=model(x,mask)\n",
        "        testloss =loss_fn(p,yactual)\n",
        "\n",
        "        testaccuracy=accuracyfun(size,q,yactual)\n",
        "        print(testaccuracy)\n",
        "        truelabs=yactual\n",
        "        L=[]\n",
        "        for i in range(size):\n",
        "          L.append(torch.argmax(q[i]))\n",
        "\n",
        "        L=np.array(L,dtype=int)\n",
        "        print(yactual.shape)\n",
        "        print(type(L[0]),type(np.array(yactual,dtype=int)[0]))\n",
        "        print(confusion_matrix(np.array(torch.argmax(yactual,axis=1)), L))\n",
        "        testaccuracy=accuracyfun(size,q,yactual,True,np.array(batch['Text']).T)\n",
        "\n",
        "        break"
      ],
      "metadata": {
        "id": "MPv2iT4FYt3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}