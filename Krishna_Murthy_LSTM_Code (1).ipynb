{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnmWAf1vb1B1"
      },
      "outputs": [],
      "source": [
        "#Importing and Modifying Training & Testing Data\n",
        "#SEE citations in other two files\n",
        "\n",
        "#Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Modules\n",
        "import gensim.models\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
        "\n",
        "#Deepti Import Data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/medical_tc_train.csv')\n",
        "testdataset = pd.read_csv('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/medical_tc_test.csv')\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "modelemb = Word2Vec.load('/content/drive/MyDrive/Senior Year/Deep Learning/Deep Learning Final Project/w2v_OA_CR_300d.bin')\n",
        "\n",
        "dataset = pd.DataFrame.to_numpy(dataset)\n",
        "testdataset = pd.DataFrame.to_numpy(testdataset)\n",
        "\n",
        "#In training dataset, make each class have 1100 abstract samples to even out classes\n",
        "dataset = dataset[dataset[:, 0].argsort()]\n",
        "datasetnew=np.copy(dataset[:5500,:])\n",
        "\n",
        "Dofstarts=dict()\n",
        "for i in range(len(dataset)):\n",
        "  row=dataset[i]\n",
        "  if int(row[0]) not in Dofstarts:\n",
        "    Dofstarts[int(row[0])] = i\n",
        "\n",
        "for i in range(5):\n",
        "  i = i+1\n",
        "  valtostart=Dofstarts[i]\n",
        "  datasetnew[(i-1)*1100:i*1100,:] = dataset[valtostart:valtostart+1100,:]\n",
        "dataset=datasetnew"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "id": "CNcYr0zK59JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.models"
      ],
      "metadata": {
        "id": "1ox1ClWL6PvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing and Modifying Training & Testing Data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#modules\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
        "\n",
        "#Sanjana Import Data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Deep Learning Project 2/medical_tc_train.csv')\n",
        "testdataset = pd.read_csv('/content/drive/MyDrive/Deep Learning Project 2/medical_tc_test.csv')\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "modelemb = Word2Vec.load('/content/drive/MyDrive/Deep Learning Project 2/w2v_OA_CR_300d.bin')\n",
        "modelemb.wv.has_index_for('diabetes')\n",
        "\n",
        "dataset = pd.DataFrame.to_numpy(dataset)\n",
        "testdataset = pd.DataFrame.to_numpy(testdataset)\n",
        "\n",
        "#in training dataset, make each class have 1100 abstract samples to even out classes\n",
        "dataset = dataset[dataset[:, 0].argsort()]\n",
        "datasetnew=np.copy(dataset[:5500,:])\n",
        "\n",
        "Dofstarts=dict()\n",
        "for i in range(len(dataset)):\n",
        "  row=dataset[i]\n",
        "  if int(row[0]) not in Dofstarts:\n",
        "    Dofstarts[int(row[0])] = i\n",
        "\n",
        "for i in range(5):\n",
        "  i = i+1\n",
        "  valtostart=Dofstarts[i]\n",
        "  datasetnew[(i-1)*1100:i*1100,:] = dataset[valtostart:valtostart+1100,:]\n",
        "\n",
        "dataset=datasetnew"
      ],
      "metadata": {
        "id": "Nb2sgmYuhGMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Tokenization\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "#tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "########## ADDED\n",
        "\n",
        "import spacy\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Create a custom tokenizer using spacy\n",
        "basic_english_tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Define a custom tokenizer that also splits on dashes\n",
        "def custom_tokenizer(text):\n",
        "    # Use the basic_english tokenizer and split on dashes\n",
        "    tokens = basic_english_tokenizer(text)\n",
        "    # Flatten the list of tokens and split on dashes\n",
        "    tokens = [subtoken for token in tokens for subtoken in token.split(\"-\")]\n",
        "    return tokens\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a test-sentence with a dash.\"\n",
        "tokens = custom_tokenizer(text)\n",
        "tokenizer=custom_tokenizer\n",
        "\n",
        "##############\n",
        "\n",
        "x,y = dataset.shape\n",
        "\n",
        "traindata = dataset[:,1]\n",
        "trainlabel = dataset[:,0]\n",
        "\n",
        "#get the minimum length of tokenized abstract sample in the training data\n",
        "min = 100000000000\n",
        "for i in range(x):\n",
        "  if len(tokenizer(traindata[i])) <min:\n",
        "    min = len(tokenizer(traindata[i]))\n",
        "\n",
        "testdata = testdataset[:,1]\n",
        "testlabel = testdataset[:,0]\n",
        "x,y = testdataset.shape\n",
        "\n",
        "#get the minimum length of tokenized abstract sample in the test data\n",
        "min = 100000000000\n",
        "for i in range(x):\n",
        "  if len(tokenizer(testdata[i])) <min:\n",
        "    min = len(tokenizer(testdata[i]))\n"
      ],
      "metadata": {
        "id": "6m043GHNfnGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing Training & Testing Data\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "#tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "WORDlen = 40\n",
        "\n",
        "class CustomTextDataset(Dataset): #gives the tokenized abstracts and corresponding labels\n",
        "    def __init__(self, txt, labels):\n",
        "        self.labels = labels\n",
        "        self.text = txt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels) #number of samples in the data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]-1\n",
        "        text = self.text[idx]\n",
        "\n",
        "        text = tokenizer(text) #tokenize the text\n",
        "        text = np.array(text) #convert to numpy array\n",
        "\n",
        "        text = text[:WORDlen] #if length is greater than WORDlen, crop tokenized text to WORDlen limit\n",
        "\n",
        "        while np.shape(text)[0] < WORDlen:\n",
        "            text = np.append(text, [\"hjgjhgjhgfgdgfdgfdfjhj\"]) #if length is less than WORDlen, build up to length\n",
        "\n",
        "        text = list(text)\n",
        "\n",
        "        sample = {\"Text\": text, \"Class\": label} #return tokenized text and labels\n",
        "        return sample\n",
        "\n",
        "traindata_set = CustomTextDataset(traindata, trainlabel)\n",
        "testdata_set = CustomTextDataset(testdata, testlabel)\n"
      ],
      "metadata": {
        "id": "XFPiE3MFi5po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e03d450-1e16-41e6-df59-ece13d2315f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Text': ['a', 'hypothesis', 'nonsteroidal', 'anti-inflammatory', 'drugs', 'reduce', 'the', 'incidence', 'of', 'large-bowel', 'cancer', '.', 'nonsteroidal', 'anti-inflammatory', 'drugs', '(', 'nsaids', ')', 'inhibit', 'prostaglandin'], 'Class': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Load --> getting shuffled batches for training\n",
        "\n",
        "trainloader = DataLoader(traindata_set, batch_size = 64, shuffle = True)\n",
        "trainloader2 = DataLoader(traindata_set, batch_size = 4000, shuffle = True)\n",
        "testloader2 = DataLoader(testdata_set, batch_size = 2000, shuffle = True)\n",
        "\n",
        "#parameters\n",
        "num_epochs = 40\n",
        "\n",
        "#for epochs in range(num_epochs):\n",
        "\"\"\"\n",
        "for i in trainloader:\n",
        "  print(i)\n",
        "\n",
        "for idx,D in enumerate(trainloader):\n",
        "    print(D)\n",
        "    break\n",
        "\"\"\"\n",
        "\n",
        "for (idx, batch) in enumerate(trainloader):\n",
        "    # Print the 'text' data of the batch\n",
        "    print(np.array(batch['Text']).T[0])\n",
        "    print(idx, 'Text data: ', (batch['Text'])[0][1])\n",
        "    # Print the 'class' data of batch\n",
        "    print(idx, 'Class data: ', len(batch['Class']), '\\n')\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHnZgU3Nlvfg",
        "outputId": "6cdca732-6c93-423b-bbad-e7082c7d493e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86\n",
            "['update' 'prostate' 'cancer' '.' 'prostate' 'cancer' 'represents' 'a'\n",
            " 'major' 'health-care' 'issue' 'for' 'older' 'men' '.' 'recent'\n",
            " 'technical' 'advances' 'in' 'imaging']\n",
            "0 Text data:  early\n",
            "0 Class data:  64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "klbi0wCo4g0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Vocabulary\n",
        "#Our current implementation of the vocabulary is all words that are seen in training and testing data.\n",
        "\n",
        "def build_vocab(first, second, WORDlen): #WORDlen = 20 (from above)\n",
        "  L = []\n",
        "\n",
        "  if first: #words from train\n",
        "    for i in traindata:\n",
        "      L.append(tokenizer(i)[:WORDlen]) #get cropped tokenized abstract\n",
        "\n",
        "  if second: #words from test\n",
        "    for i in testdata:\n",
        "      L.append(tokenizer(i)[:WORDlen]) #get cropped tokenized abstract\n",
        "\n",
        "  L=np.array(L)\n",
        "  L=np.concatenate(L)\n",
        "\n",
        "  return list(L)\n",
        "\n",
        "test_vocab_list=dict()\n",
        "train_vocab_list=dict()\n",
        "train_tokens = list(set(build_vocab(True, True, WORDlen)))\n",
        "#print(len(train_tokens))\n",
        "\n",
        "\"\"\"\n",
        "for wordlength in [50,100]:\n",
        "  z=(len(list(set(build_vocab(True,True,wordlength))))) #a+b+c\n",
        "  x=(len(list(set(build_vocab(True,False,wordlength)))))  #a+c\n",
        "  y=(len(list(set(build_vocab(False,True,wordlength))))) #b+c\n",
        "  c=x+y-z\n",
        "  print(y,y-c)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "for wordlength in [50]:\n",
        "  train_tokens_1 = list(set(build_vocab(True,False,wordlength)))\n",
        "  test_tokens_1 = list(set(build_vocab(False,True,wordlength)))\n",
        "  test_vocab_list = dict(zip(test_tokens_1, range(len(test_tokens_1))))\n",
        "  train_vocab_list = dict(zip(train_tokens_1, range(len(train_tokens_1))))\n",
        "\n",
        "count_test=0\n",
        "count_train=0\n",
        "total=0\n",
        "\n",
        "for word in test_vocab_list:\n",
        "  if not(modelemb.wv.has_index_for(word)): count_test+=1\n",
        "  total+=1\n",
        "  x = modelemb.wv.get_vector(word)\n",
        "\n",
        "  #if word not in train_vocab_list:\n",
        "  #  print(word)\n",
        "for word in train_vocab_list:\n",
        "  if not(modelemb.wv.has_index_for(word)): count_train+=1\n",
        "  total+=1\n",
        "print(count_test/total)\n",
        "print(count_train/total)\n",
        "\n",
        "\"\"\"\n",
        "#vocab list: dictionary made from vocabulary\n",
        "vocab_list = dict(zip(train_tokens, range(len(train_tokens))))\n",
        "\n",
        "'''\n",
        "vocab_list_one_hot = dict()\n",
        "for i in vocab_list:\n",
        "  length = len(vocab_list)\n",
        "  one_hot = np.zeros(length)\n",
        "  vocab_list[i]\n",
        "  one_hot[vocab_list[i]] = 1\n",
        "  vocab_list_one_hot[i] = one_hot'''\n"
      ],
      "metadata": {
        "id": "WO590LS1xuV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM Implementation\n",
        "\n",
        "import torch.nn as nn\n",
        "class MyNeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyNeuralNet,self).__init__()\n",
        "\n",
        "        #self.embedding = nn.Embedding(int(len(vocab_list)),250)\n",
        "        #self.rnn = nn.RNN(600,128,1,batch_first=True)\n",
        "\n",
        "        self.rnn = torch.nn.LSTM(300, 100, 1, batch_first=True, dropout=.2) #medical embedding size = 300\n",
        "        self.flatten = nn.Flatten(1,2)\n",
        "        inp=100\n",
        "\n",
        "        self.dropout = nn.Dropout(.3)\n",
        "        self.hid_layer = nn.Linear(inp*WORDlen, 100) #WORDlen = 20 (from above)\n",
        "        self.dropout2 = nn.Dropout(.3)\n",
        "        self.out_layer = nn.Linear(inp*WORDlen, 5)\n",
        "        self.sm = nn.Softmax(dim = 1)\n",
        "        #self.multihead_attn = nn.MultiheadAttention(embed_dim=, num_heads=2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        #x =self.embedding(x)\n",
        "        x, x2=self.rnn(x)\n",
        "        #print(x2.shape,x.shape)\n",
        "        #x = torch.swapaxes(x,0,1)\n",
        "\n",
        "        #x=x[WORDlen-1,:,:]\n",
        "        #print(x.shape)\n",
        "\n",
        "        x=self.flatten(x)\n",
        "        x=self.dropout(x)\n",
        "        #x=self.hid_layer(x)\n",
        "        #x=self.dropout2(x)\n",
        "        #attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "        x=self.out_layer(x)\n",
        "        yhat=self.sm(x)\n",
        "\n",
        "        #N 27 128  switch axes 27 N 128 --> 0 N 128\n",
        "\n",
        "        return x, yhat\n"
      ],
      "metadata": {
        "id": "pfagAcH7v4x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Training Data w Embeddings\n",
        "#After we have tokenized and shuffled batches of our training data, we need to prepare data for training in the LSTM,\n",
        "#by converting each word in the tokenized text to a 300d embedding.\n",
        "#We use the Clinical Embeddings here: https://github.com/gweissman/clinical_embeddings/blob/master/README.md.\n",
        "\n",
        "def prepareXY(batchsize,batch):\n",
        "        #x = torch.from_numpy(np.zeros((batchsize,WORDlen)))\n",
        "        #x = torch.from_numpy(np.zeros((batchsize,WORDlen,300)))\n",
        "\n",
        "        #randomized tensor to fill\n",
        "        x = (-2)*torch.rand((batchsize, WORDlen, 300))+1 #embedding size = 300, WORDlen = 20, batchsize = 64\n",
        "\n",
        "        x2 = np.array(batch['Text']).T #training data\n",
        "\n",
        "        for i2 in range(batchsize):\n",
        "          for j2 in range(WORDlen): #27 --> 206\n",
        "\n",
        "            #x[i2,j2] = vocab_list[x2[i2][j2]] #\"the\"->50\n",
        "            word=x2[i2][j2]\n",
        "            if modelemb.wv.has_index_for(word): #if there exists a clinical embedding, convert word to embedding\n",
        "              x[i2,j2] = torch.from_numpy(modelemb.wv.get_vector(word))\n",
        "\n",
        "            else: x[i2,j2]=(-2)*torch.rand(300)+1 #else, introduce a random 300d embedding for the word\n",
        "\n",
        "        x = torch.tensor(x).to(torch.float32)\n",
        "\n",
        "        y = np.array(batch['Class'])\n",
        "        y = torch.from_numpy(y.astype(int))\n",
        "\n",
        "        #one hot vectors for labels\n",
        "        yactual = []\n",
        "        for j in range(batchsize):\n",
        "          one_hot = np.zeros(5)\n",
        "          one_hot[y[j]] = 1\n",
        "          yactual.append(one_hot)\n",
        "\n",
        "        yactual = torch.Tensor(yactual)\n",
        "        return x, yactual\n",
        "\n",
        "#Get model accuracy\n",
        "def accuracyfun(batchsize,q,yactual):\n",
        "        accuracy = 0\n",
        "        for i in range(batchsize):\n",
        "          if torch.argmax(q[i]) == torch.argmax(yactual[i]):\n",
        "            accuracy += 1\n",
        "\n",
        "        accuracy/=batchsize\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "O7MG6Lg8XrEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM Training Loop\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model = MyNeuralNet()\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "trainlosses, testlosses, testaccuracies, trainaccuracies = [],[],[],[]\n",
        "\n",
        "#model.to(device)\n",
        "for epoch in range(40): #\n",
        "\n",
        "    for (idx, batch) in enumerate(trainloader):\n",
        "    # Print the 'text' data of the batch\n",
        "\n",
        "    # Print the 'class' data of batch\n",
        "        if idx>80: break\n",
        "        x,yactual = prepareXY(64,batch)\n",
        "        p,q=model(x)\n",
        "\n",
        "        loss =loss_fn(p,yactual)\n",
        "        accuracy = accuracyfun(64,q,yactual)\n",
        "\n",
        "        #print(model.state_dict())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    for idx, batch in enumerate(trainloader2):\n",
        "\n",
        "        x,yactual = prepareXY(4000, batch)\n",
        "        p,q=model(x)\n",
        "        loss =loss_fn(p,yactual)\n",
        "\n",
        "        accuracy = accuracyfun(4000, q, yactual)\n",
        "        counts=[0,0,0,0,0]\n",
        "\n",
        "        for i in range(4000):\n",
        "          counts[torch.argmax(q[i])-1]+=1\n",
        "\n",
        "        print(counts)\n",
        "        break\n",
        "\n",
        "    for idx, batch in enumerate(testloader2):\n",
        "        x,yactual = prepareXY(2000,batch)\n",
        "        p,q=model(x)\n",
        "        testloss =loss_fn(p,yactual)\n",
        "\n",
        "        testaccuracy=accuracyfun(2000,q,yactual)\n",
        "\n",
        "        break\n",
        "\n",
        "    trainlosses.append(loss.item())\n",
        "    testlosses.append(testloss.item())\n",
        "    testaccuracies.append(testaccuracy)\n",
        "    trainaccuracies.append(accuracy)\n",
        "    print(loss.item(),accuracy,testloss.item(),testaccuracy)\n",
        "\n",
        "print(\"train:\", trainlosses, testlosses)\n",
        "print(\"test:\", testaccuracies, trainaccuracies)\n"
      ],
      "metadata": {
        "id": "qb3GvbysEmyF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}